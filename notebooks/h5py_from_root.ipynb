{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import awkward as ak\n",
    "\n",
    "import h5py\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import uproot\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a651eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileConfig:\n",
    "    files = {\"../data/fourtopvsthree/rootfiles/3tJ_LO_final.root\" : 0,\n",
    "              \"../data/fourtopvsthree/rootfiles/3tWm_LO_final.root\" : 0,\n",
    "               \"../data/fourtopvsthree/rootfiles/3tWp_LO_final.root\" : 0, \n",
    "                \"../data/fourtopvsthree/rootfiles/4top_2LSS_April18.root\" : 1}\n",
    "\n",
    "PARTICLE_PREFIXES = {\n",
    "    \"jet\" : 0,\n",
    "    \"el\" : 1,\n",
    "    \"mu\" : 2\n",
    "    \n",
    "}\n",
    "\n",
    "class TopClassiferDataSetPrepaper:\n",
    "    file_config = FileConfig\n",
    "    train_split_size = 0.7\n",
    "    def __init__(self):\n",
    "        (\n",
    "         particle_data, \n",
    "         global_data, \n",
    "         src_mask,\n",
    "         targets\n",
    "        ) = self.parse_root_file(\n",
    "             max_particles = 25,\n",
    "             particle_features = [\"_pt\", \"_eta\", \"_phi\", \"_mass\"],\n",
    "             global_features = [\"met_met\", \"met_eta\", \"met_phi\"]\n",
    "         ) \n",
    "        self.data_set = TopMulitplicityClassifierDataSet(particle_data, global_data, src_mask, targets)\n",
    "        \n",
    "        self.particle_data = particle_data\n",
    "        self.global_data = global_data\n",
    "        self.targets = targets\n",
    "    \n",
    "    def parse_root_file(self, max_particles, particle_features, global_features):\n",
    "        per_file_events, per_file_targets, per_file_globals = [], [], []\n",
    "\n",
    "        for path, y in self.file_config.files.items():\n",
    "            reco = uproot.open(path)[\"Reco;1\"]\n",
    "            blocks = []\n",
    "\n",
    "            gdict = reco.arrays(global_features, how=dict)\n",
    "            gstack = ak.concatenate([gdict[name][..., None] for name in global_features], axis=-1)\n",
    "            per_file_globals.append(gstack)\n",
    "\n",
    "            for prefix in PARTICLE_PREFIXES.keys():\n",
    "                feats = [f\"{prefix}{fe}\" for fe in particle_features]\n",
    "                bdict = reco.arrays(feats, how=dict)\n",
    "                base  = ak.concatenate([bdict[name][..., None] for name in feats], axis=-1)\n",
    "\n",
    "                if prefix in (\"el\", \"mu\"):\n",
    "                    chd = reco.arrays(f\"{prefix}_charge\", how=dict)\n",
    "                    ch  = ak.concatenate([chd[k][..., None] for k in chd], axis=-1)\n",
    "                    print(ch)\n",
    "                else:\n",
    "                    ch = ak.zeros_like(base[..., :1])\n",
    "\n",
    "                if prefix == \"jet\":\n",
    "                    btd = reco.arrays(f\"{prefix}_btag\", how=dict)\n",
    "                    bt  = ak.concatenate([btd[k][..., None] for k in btd], axis=-1)\n",
    "                else:\n",
    "                    bt = ak.zeros_like(base[..., :1])\n",
    "\n",
    "                blocks.append(ak.concatenate([base, ch, bt], axis=-1))\n",
    "\n",
    "            events = ak.concatenate(blocks, axis=1)\n",
    "            per_file_events.append(events)\n",
    "            per_file_targets.append(torch.full((len(events), 1), int(y), dtype=torch.long))\n",
    "\n",
    "        global_arr = ak.concatenate(per_file_globals, axis=0)\n",
    "        global_arr = torch.from_numpy(ak.to_numpy(global_arr).astype(np.float32, copy=False))\n",
    "\n",
    "        arr = ak.concatenate(per_file_events, axis=0)                  \n",
    "        arr = ak.pad_none(arr, max_particles, axis=1, clip=True)\n",
    "\n",
    "        pad_mask_np   = ak.to_numpy(ak.is_none(arr, axis=-1))          \n",
    "        arr           = ak.fill_none(arr, np.nan)                     \n",
    "        src_mask = allnan_mask_np = ak.to_numpy(ak.all(np.isnan(arr), axis=-1))   \n",
    "\n",
    "\n",
    "        dense_np = ak.to_numpy(arr).astype(np.float32, copy=False)\n",
    "        ## Commented out to save file in h5oy format with nans\n",
    "        #np.nan_to_num(dense_np, copy=False, nan=-1010, posinf=-1010, neginf=-1010)\n",
    "        set_array = torch.from_numpy(dense_np)\n",
    "\n",
    "        target_array = torch.cat(per_file_targets, dim=0)\n",
    "        return set_array, global_arr, src_mask, target_array\n",
    "\n",
    "    def split_data_set(self):\n",
    "        self.train_data, self.val_data,  self.test_data = random_split(self.data_set,  [self.train_split_size, (1 -  self.train_split_size) /2 , (1 - self.train_split_size )/ 2])\n",
    "\n",
    "    \n",
    "\n",
    "    def _save_dataset(self, save_file_name: str):\n",
    "        ## Assumes the data set is already created tbh cba coding in the checks\n",
    "        with h5py.File(save_file_name, \"w\") as f:\n",
    "            particle_fea = f.create_group(\"particle_features\")\n",
    "            dset = particle_fea.create_dataset(\"all\", data = self.particle_data)\n",
    "\n",
    "            global_data = f.create_group(\"global_data\")\n",
    "            glo_dset = global_data.create_dataset(\"all\" , data = self.global_data)\n",
    "\n",
    "            target_data = f.create_group(\"targets\")\n",
    "            target_dset = target_data.create_dataset(\"all\", data = self.targets)\n",
    "        \n",
    "\n",
    "class TopMulitplicityClassifierDataSet(Dataset):\n",
    "    ### Torch module for Dataset, allows easy dataloader creation\n",
    "    def __init__(self, particle_features, global_features, src_mask, target_labels):\n",
    "        self.particle_features = particle_features\n",
    "        self.global_features = global_features\n",
    "        self.src_mask = src_mask\n",
    "        self.target_labels = target_labels\n",
    "    def __len__(self):\n",
    "        return self.particle_features.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"particle_features\": self.particle_features[idx],\n",
    "               \"global_features\": self.global_features[idx],\n",
    "               \"src_mask\": self.src_mask[idx]}, self.target_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe968b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1], [-1]], [], [[1]], [[-1]], [[1]], ..., [[1]], [[1]], [[-1]], [[1]]]\n",
      "[[], [[-1], [1]], [[1]], [[-1]], [[1]], ..., [[1]], [[-1]], [[-1]], [[-1]]]\n",
      "[[[-1]], [[-1]], [[-1]], [], [], ..., [[1], ...], [[-1]], [[-1], [1]], [], []]\n",
      "[[[-1]], [[1]], [[1]], [[-1], [1]], ..., [[-1]], [], [[1], [-1]], [[1], [1]]]\n",
      "[[[1], [1]], [[-1], [1]], [[-1], [1]], [], ..., [[-1], ...], [[1]], [], [[-1]]]\n",
      "[[], [], [], [[1], [1]], [[1], [-1]], ..., [], [], [[-1]], [[1], [1]], [[1]]]\n",
      "[[], [], [[1]], [], [], [[1]], [], ..., [[-1]], [], [], [], [[-1]], [], [[-1]]]\n",
      "[[[1], [1]], [[1], [1]], [[1]], [[1], ...], ..., [[-1]], [[-1], [-1]], [[-1]]]\n"
     ]
    }
   ],
   "source": [
    "top_data_sets = TopClassiferDataSetPrepaper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86644d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_data_sets._save_dataset(\"raw_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".transformer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
