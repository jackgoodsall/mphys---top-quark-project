{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff17d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import h5py\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648019f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopMulitplicityClassifierDataSet(Dataset):\n",
    "    ### Torch module for Dataset, allows easy dataloader creation\n",
    "    def __init__(self, particle_features, global_features, src_mask, target_labels):\n",
    "        self.particle_features = particle_features\n",
    "        self.global_features = global_features\n",
    "        self.src_mask = src_mask\n",
    "        self.target_labels = target_labels\n",
    "    def __len__(self):\n",
    "        return self.particle_features.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"particle_features\": self.particle_features[idx],\n",
    "               \"global_features\": self.global_features[idx],\n",
    "               \"src_mask\": self.src_mask[idx]}, self.target_labels[idx]\n",
    "\n",
    "\n",
    "class TopTensorDatasetFromH5py:\n",
    "    raw_file_name = \"/kaggle/input/threetop4toph5py/raw_data\"\n",
    "    processed_file_name =  \"processed_data\"\n",
    "    def __init__(self):\n",
    "        self._get_datas()\n",
    "        self.scale_global_data()\n",
    "        self.scale_particle_data()\n",
    "        self.src_mask = np.all(np.isnan(self.particle_data), axis = -1)\n",
    "        self.particle_data = np.nan_to_num(self.particle_data, nan = -1010)\n",
    "        \n",
    "    def scale_particle_data(self):\n",
    "        m, n = self.particle_data.shape[1] , self.particle_data.shape[2]\n",
    "        print(self.particle_data.shape)\n",
    "        data = self.particle_data.reshape(-1, m * n )\n",
    "        scaler = StandardScaler()\n",
    "        self.particle_data = scaler.fit_transform(data).reshape(-1, m, n)\n",
    "    def scale_global_data(self):\n",
    "        scaler = StandardScaler()\n",
    "        self.global_data = scaler.fit_transform(self.global_data.reshape(-1, 3)).reshape(-1, 1 , 3)\n",
    "        \n",
    "    def _get_datas(self):\n",
    "        (self.particle_data, \n",
    "        self.global_data,\n",
    "        self.targets) = self._load_file(self.raw_file_name)\n",
    "    \n",
    "    def _load_file(self, file_name):\n",
    "        with h5py.File(file_name, \"r\") as f:\n",
    "            part_data = np.array(f[\"particle_features\"][\"all\"])\n",
    "            glob_data = np.array(f[\"global_data\"][\"all\"])\n",
    "            targets = np.array(f[\"targets\"][\"all\"])\n",
    "        return part_data, glob_data, targets\n",
    "        \n",
    "    def _save_splits(self, stem: str, pad_value: float = -1010.0):\n",
    "        \"\"\"\n",
    "        Save train/val/test into separate files:\n",
    "          {stem}_train.h5, {stem}_val.h5, {stem}_test.h5\n",
    "        Uses Subset.indices to slice original numpy arrays in one shot.\n",
    "        \"\"\"\n",
    "        splits = {\"train\": self.train, \"val\": self.val, \"test\": self.test}\n",
    "\n",
    "        for name, subset in splits.items():\n",
    "            idx = np.array(subset.indices, dtype=np.int64)  # indices into original arrays\n",
    "\n",
    "            part = self.particle_data[idx]                 # [N_split, Nmax, Fp]\n",
    "            pmask = self.src_mask[idx]                     # [N_split, Nmax]\n",
    "            glob = self.global_data[idx]                   # [N_split, ...]\n",
    "            y = self.targets[idx]                          # [N_split]\n",
    "\n",
    "            out_path = f\"{stem}_{name}.h5\"\n",
    "            with h5py.File(out_path, \"w\") as f:\n",
    "                f.create_dataset(\"particle_features\", data=part,\n",
    "                                 compression=\"gzip\", compression_opts=4, chunks=True)\n",
    "                f.create_dataset(\"particle_mask\", data=pmask.astype(np.bool_),\n",
    "                                 compression=\"gzip\", compression_opts=4, chunks=True)\n",
    "                f.create_dataset(\"global_features\", data=glob,\n",
    "                                 compression=\"gzip\", compression_opts=4, chunks=True)\n",
    "                f.create_dataset(\"targets\", data=y.astype(np.int64),\n",
    "                                 compression=\"gzip\", compression_opts=4, chunks=True)\n",
    "                f.attrs[\"pad_value\"] = float(pad_value)\n",
    "            print(f\"Saved {name} -> {out_path}\")\n",
    "\n",
    "    def split_data(self, splits = [0.8, 0.1, 0.1]):\n",
    "        self.train, self.val, self.test = random_split(self.dataset, splits)\n",
    "        print(self.train[0])\n",
    "    def _load_into_tensordataset(self):\n",
    "        self.dataset = TopMulitplicityClassifierDataSet(self.particle_data, self.global_data,\n",
    "                                                       self.src_mask, self.targets)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb855e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dataset = TopTensorDatasetFromH5py()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e06485",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dataset._load_into_tensordataset()\n",
    "top_dataset.split_data()\n",
    "top_dataset._save_splits(\"/kaggle/working/processed_data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
